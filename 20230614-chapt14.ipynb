{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chap14 모델 성능 향상시키기\n",
    "### 와인 자료 활용 -> 레드와인, 화이트와인 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 : Sequential, 레이어 : Dense, 학습과 테스트 : 8:2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('./data/wine.csv', header=None)   # 데이터 불러옴\n",
    "df.head()   # 마지막 속성이 클래스\n",
    "X = df.iloc[ : , 0:-1]   # 클래스를 결정하는 속성 분리\n",
    "y = df.iloc[ : , -1]  # 클래스 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 30)                390       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 12)                372       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 104       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 875\n",
      "Trainable params: 875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 학습데이터와 테스트 데이터로 분리 -> trian_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "# 모델 생성\n",
    "model  = Sequential()\n",
    "model.add(Dense(30, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122/122 [==============================] - 2s 6ms/step - loss: 0.2336 - accuracy: 0.9184 - val_loss: 0.1811 - val_accuracy: 0.9354\n",
      "Epoch 2/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.1878 - accuracy: 0.9369 - val_loss: 0.1644 - val_accuracy: 0.9354\n",
      "Epoch 3/50\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.1629 - accuracy: 0.9456 - val_loss: 0.1485 - val_accuracy: 0.9477\n",
      "Epoch 4/50\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.1568 - accuracy: 0.9484 - val_loss: 0.1468 - val_accuracy: 0.9431\n",
      "Epoch 5/50\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.1421 - accuracy: 0.9497 - val_loss: 0.1334 - val_accuracy: 0.9485\n",
      "Epoch 6/50\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.1259 - accuracy: 0.9548 - val_loss: 0.1336 - val_accuracy: 0.9577\n",
      "Epoch 7/50\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.1214 - accuracy: 0.9564 - val_loss: 0.1148 - val_accuracy: 0.9569\n",
      "Epoch 8/50\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.1121 - accuracy: 0.9610 - val_loss: 0.1150 - val_accuracy: 0.9585\n",
      "Epoch 9/50\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.1059 - accuracy: 0.9628 - val_loss: 0.1095 - val_accuracy: 0.9608\n",
      "Epoch 10/50\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.1052 - accuracy: 0.9623 - val_loss: 0.1053 - val_accuracy: 0.9692\n",
      "Epoch 11/50\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0992 - accuracy: 0.9669 - val_loss: 0.1055 - val_accuracy: 0.9608\n",
      "Epoch 12/50\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0899 - accuracy: 0.9700 - val_loss: 0.0970 - val_accuracy: 0.9708\n",
      "Epoch 13/50\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0871 - accuracy: 0.9725 - val_loss: 0.1150 - val_accuracy: 0.9585\n",
      "Epoch 14/50\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0896 - accuracy: 0.9702 - val_loss: 0.0929 - val_accuracy: 0.9700\n",
      "Epoch 15/50\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0891 - accuracy: 0.9682 - val_loss: 0.0934 - val_accuracy: 0.9700\n",
      "Epoch 16/50\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0798 - accuracy: 0.9738 - val_loss: 0.1044 - val_accuracy: 0.9654\n",
      "Epoch 17/50\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0802 - accuracy: 0.9731 - val_loss: 0.0859 - val_accuracy: 0.9738\n",
      "Epoch 18/50\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0810 - accuracy: 0.9749 - val_loss: 0.0856 - val_accuracy: 0.9723\n",
      "Epoch 19/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0871 - accuracy: 0.9728 - val_loss: 0.1087 - val_accuracy: 0.9638\n",
      "Epoch 20/50\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0729 - accuracy: 0.9777 - val_loss: 0.0818 - val_accuracy: 0.9754\n",
      "Epoch 21/50\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0699 - accuracy: 0.9792 - val_loss: 0.0887 - val_accuracy: 0.9723\n",
      "Epoch 22/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0693 - accuracy: 0.9787 - val_loss: 0.0864 - val_accuracy: 0.9715\n",
      "Epoch 23/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0714 - accuracy: 0.9766 - val_loss: 0.0793 - val_accuracy: 0.9754\n",
      "Epoch 24/50\n",
      "122/122 [==============================] - 1s 6ms/step - loss: 0.0679 - accuracy: 0.9784 - val_loss: 0.0779 - val_accuracy: 0.9777\n",
      "Epoch 25/50\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0664 - accuracy: 0.9792 - val_loss: 0.0767 - val_accuracy: 0.9769\n",
      "Epoch 26/50\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0656 - accuracy: 0.9792 - val_loss: 0.1114 - val_accuracy: 0.9662\n",
      "Epoch 27/50\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0724 - accuracy: 0.9777 - val_loss: 0.0816 - val_accuracy: 0.9731\n",
      "Epoch 28/50\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.0670 - accuracy: 0.9802 - val_loss: 0.1475 - val_accuracy: 0.9538\n",
      "Epoch 29/50\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0626 - accuracy: 0.9805 - val_loss: 0.0863 - val_accuracy: 0.9715\n",
      "Epoch 30/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0734 - accuracy: 0.9761 - val_loss: 0.0753 - val_accuracy: 0.9785\n",
      "Epoch 31/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0776 - accuracy: 0.9761 - val_loss: 0.0768 - val_accuracy: 0.9762\n",
      "Epoch 32/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0622 - accuracy: 0.9805 - val_loss: 0.0772 - val_accuracy: 0.9746\n",
      "Epoch 33/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0604 - accuracy: 0.9833 - val_loss: 0.0945 - val_accuracy: 0.9715\n",
      "Epoch 34/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0777 - accuracy: 0.9751 - val_loss: 0.0876 - val_accuracy: 0.9685\n",
      "Epoch 35/50\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0588 - accuracy: 0.9820 - val_loss: 0.0732 - val_accuracy: 0.9769\n",
      "Epoch 36/50\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0571 - accuracy: 0.9820 - val_loss: 0.0690 - val_accuracy: 0.9792\n",
      "Epoch 37/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0643 - accuracy: 0.9805 - val_loss: 0.1024 - val_accuracy: 0.9692\n",
      "Epoch 38/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0575 - accuracy: 0.9828 - val_loss: 0.0689 - val_accuracy: 0.9792\n",
      "Epoch 39/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0564 - accuracy: 0.9826 - val_loss: 0.0958 - val_accuracy: 0.9669\n",
      "Epoch 40/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0606 - accuracy: 0.9826 - val_loss: 0.0707 - val_accuracy: 0.9777\n",
      "Epoch 41/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0600 - accuracy: 0.9826 - val_loss: 0.0812 - val_accuracy: 0.9769\n",
      "Epoch 42/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0667 - accuracy: 0.9802 - val_loss: 0.0700 - val_accuracy: 0.9785\n",
      "Epoch 43/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0621 - accuracy: 0.9823 - val_loss: 0.0756 - val_accuracy: 0.9754\n",
      "Epoch 44/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0616 - accuracy: 0.9787 - val_loss: 0.0651 - val_accuracy: 0.9808\n",
      "Epoch 45/50\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.0540 - accuracy: 0.9833 - val_loss: 0.0654 - val_accuracy: 0.9800\n",
      "Epoch 46/50\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.0597 - accuracy: 0.9802 - val_loss: 0.0798 - val_accuracy: 0.9754\n",
      "Epoch 47/50\n",
      "122/122 [==============================] - 1s 8ms/step - loss: 0.0566 - accuracy: 0.9838 - val_loss: 0.0645 - val_accuracy: 0.9800\n",
      "Epoch 48/50\n",
      "122/122 [==============================] - 1s 7ms/step - loss: 0.0591 - accuracy: 0.9836 - val_loss: 0.0638 - val_accuracy: 0.9800\n",
      "Epoch 49/50\n",
      "122/122 [==============================] - 1s 5ms/step - loss: 0.0557 - accuracy: 0.9818 - val_loss: 0.0653 - val_accuracy: 0.9785\n",
      "Epoch 50/50\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0560 - accuracy: 0.9836 - val_loss: 0.0672 - val_accuracy: 0.9808\n"
     ]
    }
   ],
   "source": [
    "# 모델 컴파일 : loss, optimizer, metrics\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 모델 실행 \n",
    "history = model.fit(X_train, y_train, epochs=50, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9353846311569214,\n",
       " 0.9353846311569214,\n",
       " 0.947692334651947,\n",
       " 0.9430769085884094,\n",
       " 0.9484615325927734,\n",
       " 0.9576923251152039,\n",
       " 0.9569230675697327,\n",
       " 0.9584615230560303,\n",
       " 0.9607692360877991,\n",
       " 0.9692307710647583,\n",
       " 0.9607692360877991,\n",
       " 0.9707692265510559,\n",
       " 0.9584615230560303,\n",
       " 0.9700000286102295,\n",
       " 0.9700000286102295,\n",
       " 0.9653846025466919,\n",
       " 0.9738461375236511,\n",
       " 0.9723076820373535,\n",
       " 0.9638461470603943,\n",
       " 0.9753845930099487,\n",
       " 0.9723076820373535,\n",
       " 0.9715384840965271,\n",
       " 0.9753845930099487,\n",
       " 0.9776923060417175,\n",
       " 0.9769230484962463,\n",
       " 0.9661538600921631,\n",
       " 0.9730769395828247,\n",
       " 0.9538461565971375,\n",
       " 0.9715384840965271,\n",
       " 0.9784615635871887,\n",
       " 0.9761538505554199,\n",
       " 0.9746153950691223,\n",
       " 0.9715384840965271,\n",
       " 0.9684615135192871,\n",
       " 0.9769230484962463,\n",
       " 0.9792307615280151,\n",
       " 0.9692307710647583,\n",
       " 0.9792307615280151,\n",
       " 0.9669230580329895,\n",
       " 0.9776923060417175,\n",
       " 0.9769230484962463,\n",
       " 0.9784615635871887,\n",
       " 0.9753845930099487,\n",
       " 0.9807692170143127,\n",
       " 0.9800000190734863,\n",
       " 0.9753845930099487,\n",
       " 0.9800000190734863,\n",
       " 0.9800000190734863,\n",
       " 0.9784615635871887,\n",
       " 0.9807692170143127]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to ./data/model\\01-0.7646.hdf5\n",
      "\n",
      "Epoch 2: saving model to ./data/model\\02-0.7715.hdf5\n",
      "\n",
      "Epoch 3: saving model to ./data/model\\03-0.7962.hdf5\n",
      "\n",
      "Epoch 4: saving model to ./data/model\\04-0.8646.hdf5\n",
      "\n",
      "Epoch 5: saving model to ./data/model\\05-0.8900.hdf5\n",
      "\n",
      "Epoch 6: saving model to ./data/model\\06-0.9246.hdf5\n",
      "\n",
      "Epoch 7: saving model to ./data/model\\07-0.9300.hdf5\n",
      "\n",
      "Epoch 8: saving model to ./data/model\\08-0.9377.hdf5\n",
      "\n",
      "Epoch 9: saving model to ./data/model\\09-0.9369.hdf5\n",
      "\n",
      "Epoch 10: saving model to ./data/model\\10-0.9369.hdf5\n",
      "\n",
      "Epoch 11: saving model to ./data/model\\11-0.9415.hdf5\n",
      "\n",
      "Epoch 12: saving model to ./data/model\\12-0.9392.hdf5\n",
      "\n",
      "Epoch 13: saving model to ./data/model\\13-0.9392.hdf5\n",
      "\n",
      "Epoch 14: saving model to ./data/model\\14-0.9385.hdf5\n",
      "\n",
      "Epoch 15: saving model to ./data/model\\15-0.9400.hdf5\n",
      "\n",
      "Epoch 16: saving model to ./data/model\\16-0.9400.hdf5\n",
      "\n",
      "Epoch 17: saving model to ./data/model\\17-0.9400.hdf5\n",
      "\n",
      "Epoch 18: saving model to ./data/model\\18-0.9385.hdf5\n",
      "\n",
      "Epoch 19: saving model to ./data/model\\19-0.9415.hdf5\n",
      "\n",
      "Epoch 20: saving model to ./data/model\\20-0.9385.hdf5\n",
      "\n",
      "Epoch 21: saving model to ./data/model\\21-0.9408.hdf5\n",
      "\n",
      "Epoch 22: saving model to ./data/model\\22-0.9415.hdf5\n",
      "\n",
      "Epoch 23: saving model to ./data/model\\23-0.9431.hdf5\n",
      "\n",
      "Epoch 24: saving model to ./data/model\\24-0.9423.hdf5\n",
      "\n",
      "Epoch 25: saving model to ./data/model\\25-0.9408.hdf5\n",
      "\n",
      "Epoch 26: saving model to ./data/model\\26-0.9415.hdf5\n",
      "\n",
      "Epoch 27: saving model to ./data/model\\27-0.9477.hdf5\n",
      "\n",
      "Epoch 28: saving model to ./data/model\\28-0.9477.hdf5\n",
      "\n",
      "Epoch 29: saving model to ./data/model\\29-0.9462.hdf5\n",
      "\n",
      "Epoch 30: saving model to ./data/model\\30-0.9469.hdf5\n",
      "\n",
      "Epoch 31: saving model to ./data/model\\31-0.9446.hdf5\n",
      "\n",
      "Epoch 32: saving model to ./data/model\\32-0.9462.hdf5\n",
      "\n",
      "Epoch 33: saving model to ./data/model\\33-0.9454.hdf5\n",
      "\n",
      "Epoch 34: saving model to ./data/model\\34-0.9469.hdf5\n",
      "\n",
      "Epoch 35: saving model to ./data/model\\35-0.9469.hdf5\n",
      "\n",
      "Epoch 36: saving model to ./data/model\\36-0.9462.hdf5\n",
      "\n",
      "Epoch 37: saving model to ./data/model\\37-0.9485.hdf5\n",
      "\n",
      "Epoch 38: saving model to ./data/model\\38-0.9469.hdf5\n",
      "\n",
      "Epoch 39: saving model to ./data/model\\39-0.9500.hdf5\n",
      "\n",
      "Epoch 40: saving model to ./data/model\\40-0.9492.hdf5\n",
      "\n",
      "Epoch 41: saving model to ./data/model\\41-0.9492.hdf5\n",
      "\n",
      "Epoch 42: saving model to ./data/model\\42-0.9500.hdf5\n",
      "\n",
      "Epoch 43: saving model to ./data/model\\43-0.9485.hdf5\n",
      "\n",
      "Epoch 44: saving model to ./data/model\\44-0.9500.hdf5\n",
      "\n",
      "Epoch 45: saving model to ./data/model\\45-0.9485.hdf5\n",
      "\n",
      "Epoch 46: saving model to ./data/model\\46-0.9500.hdf5\n",
      "\n",
      "Epoch 47: saving model to ./data/model\\47-0.9485.hdf5\n",
      "\n",
      "Epoch 48: saving model to ./data/model\\48-0.9515.hdf5\n",
      "\n",
      "Epoch 49: saving model to ./data/model\\49-0.9500.hdf5\n",
      "\n",
      "Epoch 50: saving model to ./data/model\\50-0.9515.hdf5\n",
      "41/41 [==============================] - 0s 4ms/step - loss: 0.1409 - accuracy: 0.9438\n",
      "Test accuracy :  0.9438461661338806\n"
     ]
    }
   ],
   "source": [
    "# 모델 엡데이트 하면서 모델 학습 실행\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filename = './data/model/{epoch:02d}-{val_accuracy:.4f}.hdf5'\n",
    "\n",
    "# 학습 중에 모델 저장\n",
    "checkpointer = ModelCheckpoint(filepath=filename, verbose=True) \n",
    "\n",
    "# 모델 생성\n",
    "model = Sequential()\n",
    "model.add(Dense(24,input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(12,activation='relu'))\n",
    "model.add(Dense(8,activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# 모델 컴파일\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# 모델 실행 \n",
    "hist = model.fit(X_train, y_train, epochs=50, validation_split=0.25, \n",
    "                 verbose=0,batch_size=500,callbacks=[checkpointer])\n",
    "\n",
    "# 테스트 결과 출력\n",
    "score = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy : ', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습을 2000회 실행\n",
    "model = Sequential()\n",
    "model.add(Dense(24,input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(12,activation='relu'))\n",
    "model.add(Dense(8,activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# 모델 컴파일\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# 모델 실행 \n",
    "hist = model.fit(X_train, y_train, epochs=2000, validation_split=0.25, \n",
    "                 verbose=0,batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 실행한 결과를 그래프로 확인 -> 과적합 여부 ?\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "hist_df = pd.DataFrame(hist.history)\n",
    "hist_df.head()   # loss, val_loss 의 데이터를 추출\n",
    "y_loss = hist_df.loss\n",
    "y_vloss = hist_df['val_loss']\n",
    "x_len = np.arange(len(y_loss))\n",
    "\n",
    "plt.plot(x_len,y_loss, \"o\", c='blue',markersize=1, label='Train_loss' )\n",
    "plt.plot(x_len,y_vloss, \"o\", c='red', markersize=1, label='Test_loss' )\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "8/8 [==============================] - 3s 97ms/step - loss: 2.8175 - accuracy: 0.7549 - val_loss: 2.1983 - val_accuracy: 0.7546\n",
      "Epoch 2/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.9804 - accuracy: 0.7549 - val_loss: 1.5174 - val_accuracy: 0.7546\n",
      "Epoch 3/2000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 1.3360 - accuracy: 0.7549 - val_loss: 0.9783 - val_accuracy: 0.7546\n",
      "Epoch 4/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.8121 - accuracy: 0.7549 - val_loss: 0.5564 - val_accuracy: 0.7546\n",
      "Epoch 5/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.4929 - accuracy: 0.7549 - val_loss: 0.4969 - val_accuracy: 0.7546\n",
      "Epoch 6/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4717 - accuracy: 0.7549 - val_loss: 0.4119 - val_accuracy: 0.7546\n",
      "Epoch 7/2000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 0.3900 - accuracy: 0.7549 - val_loss: 0.3769 - val_accuracy: 0.7546\n",
      "Epoch 8/2000\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.3754 - accuracy: 0.7549 - val_loss: 0.3544 - val_accuracy: 0.7546\n",
      "Epoch 9/2000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.3503 - accuracy: 0.7580 - val_loss: 0.3391 - val_accuracy: 0.7785\n",
      "Epoch 10/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.3378 - accuracy: 0.7952 - val_loss: 0.3261 - val_accuracy: 0.8215\n",
      "Epoch 11/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.3226 - accuracy: 0.8391 - val_loss: 0.3112 - val_accuracy: 0.8585\n",
      "Epoch 12/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.3122 - accuracy: 0.8661 - val_loss: 0.3006 - val_accuracy: 0.8831\n",
      "Epoch 13/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.3025 - accuracy: 0.8922 - val_loss: 0.2907 - val_accuracy: 0.9069\n",
      "Epoch 14/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.2918 - accuracy: 0.9120 - val_loss: 0.2790 - val_accuracy: 0.9269\n",
      "Epoch 15/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.2806 - accuracy: 0.9228 - val_loss: 0.2697 - val_accuracy: 0.9331\n",
      "Epoch 16/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.2745 - accuracy: 0.9276 - val_loss: 0.2629 - val_accuracy: 0.9338\n",
      "Epoch 17/2000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.2694 - accuracy: 0.9287 - val_loss: 0.2573 - val_accuracy: 0.9331\n",
      "Epoch 18/2000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.2648 - accuracy: 0.9302 - val_loss: 0.2522 - val_accuracy: 0.9323\n",
      "Epoch 19/2000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.2596 - accuracy: 0.9325 - val_loss: 0.2466 - val_accuracy: 0.9331\n",
      "Epoch 20/2000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.2545 - accuracy: 0.9320 - val_loss: 0.2413 - val_accuracy: 0.9338\n",
      "Epoch 21/2000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.2489 - accuracy: 0.9338 - val_loss: 0.2363 - val_accuracy: 0.9377\n",
      "Epoch 22/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.2436 - accuracy: 0.9356 - val_loss: 0.2313 - val_accuracy: 0.9377\n",
      "Epoch 23/2000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.2385 - accuracy: 0.9356 - val_loss: 0.2258 - val_accuracy: 0.9377\n",
      "Epoch 24/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.2334 - accuracy: 0.9353 - val_loss: 0.2209 - val_accuracy: 0.9377\n",
      "Epoch 25/2000\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.2288 - accuracy: 0.9353 - val_loss: 0.2167 - val_accuracy: 0.9354\n",
      "Epoch 26/2000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.2245 - accuracy: 0.9374 - val_loss: 0.2123 - val_accuracy: 0.9354\n",
      "Epoch 27/2000\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.2205 - accuracy: 0.9376 - val_loss: 0.2087 - val_accuracy: 0.9354\n",
      "Epoch 28/2000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.2167 - accuracy: 0.9387 - val_loss: 0.2047 - val_accuracy: 0.9354\n",
      "Epoch 29/2000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.2131 - accuracy: 0.9384 - val_loss: 0.2014 - val_accuracy: 0.9362\n",
      "Epoch 30/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.2096 - accuracy: 0.9392 - val_loss: 0.1982 - val_accuracy: 0.9362\n",
      "Epoch 31/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.2062 - accuracy: 0.9397 - val_loss: 0.1952 - val_accuracy: 0.9354\n",
      "Epoch 32/2000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.2031 - accuracy: 0.9392 - val_loss: 0.1923 - val_accuracy: 0.9354\n",
      "Epoch 33/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.2007 - accuracy: 0.9394 - val_loss: 0.1900 - val_accuracy: 0.9362\n",
      "Epoch 34/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1982 - accuracy: 0.9400 - val_loss: 0.1877 - val_accuracy: 0.9346\n",
      "Epoch 35/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.1963 - accuracy: 0.9397 - val_loss: 0.1862 - val_accuracy: 0.9346\n",
      "Epoch 36/2000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.1945 - accuracy: 0.9392 - val_loss: 0.1845 - val_accuracy: 0.9338\n",
      "Epoch 37/2000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.1930 - accuracy: 0.9394 - val_loss: 0.1833 - val_accuracy: 0.9377\n",
      "Epoch 38/2000\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.1914 - accuracy: 0.9397 - val_loss: 0.1816 - val_accuracy: 0.9354\n",
      "Epoch 39/2000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 0.1902 - accuracy: 0.9400 - val_loss: 0.1808 - val_accuracy: 0.9354\n",
      "Epoch 40/2000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 0.1887 - accuracy: 0.9397 - val_loss: 0.1792 - val_accuracy: 0.9362\n",
      "Epoch 41/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.1877 - accuracy: 0.9397 - val_loss: 0.1783 - val_accuracy: 0.9354\n",
      "Epoch 42/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1866 - accuracy: 0.9397 - val_loss: 0.1776 - val_accuracy: 0.9354\n",
      "Epoch 43/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.1855 - accuracy: 0.9397 - val_loss: 0.1762 - val_accuracy: 0.9362\n",
      "Epoch 44/2000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.1847 - accuracy: 0.9397 - val_loss: 0.1753 - val_accuracy: 0.9369\n",
      "Epoch 45/2000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.1836 - accuracy: 0.9394 - val_loss: 0.1744 - val_accuracy: 0.9377\n",
      "Epoch 46/2000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.1829 - accuracy: 0.9402 - val_loss: 0.1734 - val_accuracy: 0.9385\n",
      "Epoch 47/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.1820 - accuracy: 0.9402 - val_loss: 0.1726 - val_accuracy: 0.9385\n",
      "Epoch 48/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1810 - accuracy: 0.9410 - val_loss: 0.1723 - val_accuracy: 0.9385\n",
      "Epoch 49/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.1802 - accuracy: 0.9415 - val_loss: 0.1712 - val_accuracy: 0.9385\n",
      "Epoch 50/2000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.1801 - accuracy: 0.9402 - val_loss: 0.1707 - val_accuracy: 0.9385\n",
      "Epoch 51/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.1793 - accuracy: 0.9418 - val_loss: 0.1705 - val_accuracy: 0.9392\n",
      "Epoch 52/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.1781 - accuracy: 0.9407 - val_loss: 0.1691 - val_accuracy: 0.9392\n",
      "Epoch 53/2000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.1774 - accuracy: 0.9405 - val_loss: 0.1686 - val_accuracy: 0.9392\n",
      "Epoch 54/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.1766 - accuracy: 0.9405 - val_loss: 0.1675 - val_accuracy: 0.9400\n",
      "Epoch 55/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1759 - accuracy: 0.9418 - val_loss: 0.1675 - val_accuracy: 0.9392\n",
      "Epoch 56/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1749 - accuracy: 0.9423 - val_loss: 0.1658 - val_accuracy: 0.9400\n",
      "Epoch 57/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.1744 - accuracy: 0.9418 - val_loss: 0.1658 - val_accuracy: 0.9400\n",
      "Epoch 58/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.1736 - accuracy: 0.9423 - val_loss: 0.1644 - val_accuracy: 0.9400\n",
      "Epoch 59/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.1728 - accuracy: 0.9425 - val_loss: 0.1642 - val_accuracy: 0.9408\n",
      "Epoch 60/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1719 - accuracy: 0.9420 - val_loss: 0.1631 - val_accuracy: 0.9408\n",
      "Epoch 61/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1712 - accuracy: 0.9428 - val_loss: 0.1628 - val_accuracy: 0.9408\n",
      "Epoch 62/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1703 - accuracy: 0.9433 - val_loss: 0.1623 - val_accuracy: 0.9408\n",
      "Epoch 63/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1696 - accuracy: 0.9433 - val_loss: 0.1609 - val_accuracy: 0.9408\n",
      "Epoch 64/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1691 - accuracy: 0.9430 - val_loss: 0.1604 - val_accuracy: 0.9415\n",
      "Epoch 65/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1682 - accuracy: 0.9446 - val_loss: 0.1598 - val_accuracy: 0.9423\n",
      "Epoch 66/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1676 - accuracy: 0.9435 - val_loss: 0.1588 - val_accuracy: 0.9423\n",
      "Epoch 67/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1666 - accuracy: 0.9446 - val_loss: 0.1587 - val_accuracy: 0.9431\n",
      "Epoch 68/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1660 - accuracy: 0.9448 - val_loss: 0.1583 - val_accuracy: 0.9423\n",
      "Epoch 69/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1661 - accuracy: 0.9451 - val_loss: 0.1572 - val_accuracy: 0.9431\n",
      "Epoch 70/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1647 - accuracy: 0.9435 - val_loss: 0.1567 - val_accuracy: 0.9431\n",
      "Epoch 71/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1635 - accuracy: 0.9453 - val_loss: 0.1556 - val_accuracy: 0.9454\n",
      "Epoch 72/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1628 - accuracy: 0.9451 - val_loss: 0.1553 - val_accuracy: 0.9431\n",
      "Epoch 73/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1625 - accuracy: 0.9446 - val_loss: 0.1540 - val_accuracy: 0.9446\n",
      "Epoch 74/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1618 - accuracy: 0.9459 - val_loss: 0.1538 - val_accuracy: 0.9438\n",
      "Epoch 75/2000\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.1607 - accuracy: 0.9443 - val_loss: 0.1525 - val_accuracy: 0.9446\n",
      "Epoch 76/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.1596 - accuracy: 0.9453 - val_loss: 0.1524 - val_accuracy: 0.9446\n",
      "Epoch 77/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1592 - accuracy: 0.9451 - val_loss: 0.1517 - val_accuracy: 0.9454\n",
      "Epoch 78/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1583 - accuracy: 0.9461 - val_loss: 0.1507 - val_accuracy: 0.9454\n",
      "Epoch 79/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1571 - accuracy: 0.9451 - val_loss: 0.1508 - val_accuracy: 0.9462\n",
      "Epoch 80/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1564 - accuracy: 0.9453 - val_loss: 0.1496 - val_accuracy: 0.9462\n",
      "Epoch 81/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1558 - accuracy: 0.9461 - val_loss: 0.1502 - val_accuracy: 0.9462\n",
      "Epoch 82/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1554 - accuracy: 0.9469 - val_loss: 0.1481 - val_accuracy: 0.9469\n",
      "Epoch 83/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1540 - accuracy: 0.9464 - val_loss: 0.1485 - val_accuracy: 0.9462\n",
      "Epoch 84/2000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.1545 - accuracy: 0.9464 - val_loss: 0.1488 - val_accuracy: 0.9469\n",
      "Epoch 85/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1529 - accuracy: 0.9466 - val_loss: 0.1476 - val_accuracy: 0.9469\n",
      "Epoch 86/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1514 - accuracy: 0.9474 - val_loss: 0.1464 - val_accuracy: 0.9469\n",
      "Epoch 87/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1505 - accuracy: 0.9471 - val_loss: 0.1458 - val_accuracy: 0.9469\n",
      "Epoch 88/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1501 - accuracy: 0.9487 - val_loss: 0.1452 - val_accuracy: 0.9469\n",
      "Epoch 89/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1487 - accuracy: 0.9482 - val_loss: 0.1440 - val_accuracy: 0.9469\n",
      "Epoch 90/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1481 - accuracy: 0.9487 - val_loss: 0.1437 - val_accuracy: 0.9477\n",
      "Epoch 91/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1470 - accuracy: 0.9487 - val_loss: 0.1421 - val_accuracy: 0.9469\n",
      "Epoch 92/2000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 0.1457 - accuracy: 0.9505 - val_loss: 0.1421 - val_accuracy: 0.9485\n",
      "Epoch 93/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.1441 - accuracy: 0.9500 - val_loss: 0.1408 - val_accuracy: 0.9485\n",
      "Epoch 94/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1430 - accuracy: 0.9507 - val_loss: 0.1414 - val_accuracy: 0.9469\n",
      "Epoch 95/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1417 - accuracy: 0.9510 - val_loss: 0.1399 - val_accuracy: 0.9492\n",
      "Epoch 96/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1407 - accuracy: 0.9502 - val_loss: 0.1425 - val_accuracy: 0.9469\n",
      "Epoch 97/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.1398 - accuracy: 0.9512 - val_loss: 0.1383 - val_accuracy: 0.9500\n",
      "Epoch 98/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1378 - accuracy: 0.9515 - val_loss: 0.1383 - val_accuracy: 0.9462\n",
      "Epoch 99/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1358 - accuracy: 0.9515 - val_loss: 0.1353 - val_accuracy: 0.9485\n",
      "Epoch 100/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1340 - accuracy: 0.9520 - val_loss: 0.1348 - val_accuracy: 0.9500\n",
      "Epoch 101/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1324 - accuracy: 0.9533 - val_loss: 0.1345 - val_accuracy: 0.9500\n",
      "Epoch 102/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1315 - accuracy: 0.9520 - val_loss: 0.1323 - val_accuracy: 0.9508\n",
      "Epoch 103/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1296 - accuracy: 0.9530 - val_loss: 0.1313 - val_accuracy: 0.9508\n",
      "Epoch 104/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1282 - accuracy: 0.9541 - val_loss: 0.1313 - val_accuracy: 0.9500\n",
      "Epoch 105/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1270 - accuracy: 0.9538 - val_loss: 0.1290 - val_accuracy: 0.9515\n",
      "Epoch 106/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1261 - accuracy: 0.9543 - val_loss: 0.1290 - val_accuracy: 0.9515\n",
      "Epoch 107/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1267 - accuracy: 0.9538 - val_loss: 0.1275 - val_accuracy: 0.9508\n",
      "Epoch 108/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1249 - accuracy: 0.9530 - val_loss: 0.1275 - val_accuracy: 0.9508\n",
      "Epoch 109/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1231 - accuracy: 0.9538 - val_loss: 0.1255 - val_accuracy: 0.9508\n",
      "Epoch 110/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1227 - accuracy: 0.9546 - val_loss: 0.1283 - val_accuracy: 0.9492\n",
      "Epoch 111/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1209 - accuracy: 0.9546 - val_loss: 0.1254 - val_accuracy: 0.9523\n",
      "Epoch 112/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1202 - accuracy: 0.9546 - val_loss: 0.1240 - val_accuracy: 0.9538\n",
      "Epoch 113/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1206 - accuracy: 0.9548 - val_loss: 0.1299 - val_accuracy: 0.9508\n",
      "Epoch 114/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1186 - accuracy: 0.9551 - val_loss: 0.1240 - val_accuracy: 0.9515\n",
      "Epoch 115/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1176 - accuracy: 0.9554 - val_loss: 0.1218 - val_accuracy: 0.9523\n",
      "Epoch 116/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1165 - accuracy: 0.9569 - val_loss: 0.1210 - val_accuracy: 0.9531\n",
      "Epoch 117/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1160 - accuracy: 0.9561 - val_loss: 0.1216 - val_accuracy: 0.9531\n",
      "Epoch 118/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1155 - accuracy: 0.9564 - val_loss: 0.1287 - val_accuracy: 0.9554\n",
      "Epoch 119/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1169 - accuracy: 0.9566 - val_loss: 0.1225 - val_accuracy: 0.9523\n",
      "Epoch 120/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1131 - accuracy: 0.9592 - val_loss: 0.1179 - val_accuracy: 0.9546\n",
      "Epoch 121/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1106 - accuracy: 0.9602 - val_loss: 0.1181 - val_accuracy: 0.9569\n",
      "Epoch 122/2000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.1102 - accuracy: 0.9607 - val_loss: 0.1169 - val_accuracy: 0.9569\n",
      "Epoch 123/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.1097 - accuracy: 0.9589 - val_loss: 0.1206 - val_accuracy: 0.9569\n",
      "Epoch 124/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1080 - accuracy: 0.9610 - val_loss: 0.1160 - val_accuracy: 0.9577\n",
      "Epoch 125/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1070 - accuracy: 0.9618 - val_loss: 0.1149 - val_accuracy: 0.9569\n",
      "Epoch 126/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1059 - accuracy: 0.9615 - val_loss: 0.1159 - val_accuracy: 0.9569\n",
      "Epoch 127/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1052 - accuracy: 0.9630 - val_loss: 0.1138 - val_accuracy: 0.9585\n",
      "Epoch 128/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1040 - accuracy: 0.9648 - val_loss: 0.1138 - val_accuracy: 0.9577\n",
      "Epoch 129/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1032 - accuracy: 0.9643 - val_loss: 0.1129 - val_accuracy: 0.9577\n",
      "Epoch 130/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1023 - accuracy: 0.9659 - val_loss: 0.1165 - val_accuracy: 0.9608\n",
      "Epoch 131/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1018 - accuracy: 0.9654 - val_loss: 0.1121 - val_accuracy: 0.9592\n",
      "Epoch 132/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1003 - accuracy: 0.9672 - val_loss: 0.1108 - val_accuracy: 0.9600\n",
      "Epoch 133/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0996 - accuracy: 0.9661 - val_loss: 0.1111 - val_accuracy: 0.9615\n",
      "Epoch 134/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0986 - accuracy: 0.9672 - val_loss: 0.1099 - val_accuracy: 0.9615\n",
      "Epoch 135/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0970 - accuracy: 0.9692 - val_loss: 0.1080 - val_accuracy: 0.9592\n",
      "Epoch 136/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0969 - accuracy: 0.9697 - val_loss: 0.1074 - val_accuracy: 0.9631\n",
      "Epoch 137/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0961 - accuracy: 0.9684 - val_loss: 0.1069 - val_accuracy: 0.9615\n",
      "Epoch 138/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0962 - accuracy: 0.9677 - val_loss: 0.1062 - val_accuracy: 0.9608\n",
      "Epoch 139/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0944 - accuracy: 0.9707 - val_loss: 0.1057 - val_accuracy: 0.9615\n",
      "Epoch 140/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0938 - accuracy: 0.9707 - val_loss: 0.1044 - val_accuracy: 0.9638\n",
      "Epoch 141/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0934 - accuracy: 0.9702 - val_loss: 0.1052 - val_accuracy: 0.9631\n",
      "Epoch 142/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0921 - accuracy: 0.9690 - val_loss: 0.1089 - val_accuracy: 0.9631\n",
      "Epoch 143/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0935 - accuracy: 0.9682 - val_loss: 0.1046 - val_accuracy: 0.9646\n",
      "Epoch 144/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0944 - accuracy: 0.9697 - val_loss: 0.1039 - val_accuracy: 0.9631\n",
      "Epoch 145/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0912 - accuracy: 0.9705 - val_loss: 0.1014 - val_accuracy: 0.9662\n",
      "Epoch 146/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0891 - accuracy: 0.9731 - val_loss: 0.1011 - val_accuracy: 0.9654\n",
      "Epoch 147/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0887 - accuracy: 0.9728 - val_loss: 0.1048 - val_accuracy: 0.9638\n",
      "Epoch 148/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0890 - accuracy: 0.9705 - val_loss: 0.1051 - val_accuracy: 0.9631\n",
      "Epoch 149/2000\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 0.0885 - accuracy: 0.9731 - val_loss: 0.0997 - val_accuracy: 0.9646\n",
      "Epoch 150/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0902 - accuracy: 0.9718 - val_loss: 0.0989 - val_accuracy: 0.9662\n",
      "Epoch 151/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0897 - accuracy: 0.9718 - val_loss: 0.1006 - val_accuracy: 0.9623\n",
      "Epoch 152/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0886 - accuracy: 0.9736 - val_loss: 0.1020 - val_accuracy: 0.9615\n",
      "Epoch 153/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0887 - accuracy: 0.9713 - val_loss: 0.0974 - val_accuracy: 0.9662\n",
      "Epoch 154/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0858 - accuracy: 0.9728 - val_loss: 0.0989 - val_accuracy: 0.9662\n",
      "Epoch 155/2000\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0859 - accuracy: 0.9738 - val_loss: 0.0999 - val_accuracy: 0.9662\n",
      "Epoch 156/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0841 - accuracy: 0.9736 - val_loss: 0.0964 - val_accuracy: 0.9646\n",
      "Epoch 157/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0841 - accuracy: 0.9741 - val_loss: 0.0954 - val_accuracy: 0.9677\n",
      "Epoch 158/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0818 - accuracy: 0.9754 - val_loss: 0.0964 - val_accuracy: 0.9654\n",
      "Epoch 159/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0819 - accuracy: 0.9759 - val_loss: 0.0958 - val_accuracy: 0.9654\n",
      "Epoch 160/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0821 - accuracy: 0.9761 - val_loss: 0.0953 - val_accuracy: 0.9662\n",
      "Epoch 161/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0806 - accuracy: 0.9754 - val_loss: 0.0940 - val_accuracy: 0.9662\n",
      "Epoch 162/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0797 - accuracy: 0.9764 - val_loss: 0.0931 - val_accuracy: 0.9685\n",
      "Epoch 163/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0795 - accuracy: 0.9766 - val_loss: 0.0945 - val_accuracy: 0.9669\n",
      "Epoch 164/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0808 - accuracy: 0.9751 - val_loss: 0.0923 - val_accuracy: 0.9700\n",
      "Epoch 165/2000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 0.0805 - accuracy: 0.9761 - val_loss: 0.0922 - val_accuracy: 0.9669\n",
      "Epoch 166/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0777 - accuracy: 0.9766 - val_loss: 0.0923 - val_accuracy: 0.9669\n",
      "Epoch 167/2000\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.0780 - accuracy: 0.9779 - val_loss: 0.0915 - val_accuracy: 0.9700\n",
      "Epoch 168/2000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.0800 - accuracy: 0.9759 - val_loss: 0.0911 - val_accuracy: 0.9692\n",
      "Epoch 169/2000\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.0820 - accuracy: 0.9733 - val_loss: 0.0919 - val_accuracy: 0.9685\n",
      "Epoch 170/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0774 - accuracy: 0.9787 - val_loss: 0.0917 - val_accuracy: 0.9685\n",
      "Epoch 171/2000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 0.0764 - accuracy: 0.9782 - val_loss: 0.0901 - val_accuracy: 0.9700\n",
      "Epoch 172/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0769 - accuracy: 0.9766 - val_loss: 0.0906 - val_accuracy: 0.9685\n",
      "Epoch 173/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0747 - accuracy: 0.9774 - val_loss: 0.0887 - val_accuracy: 0.9715\n",
      "Epoch 174/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0743 - accuracy: 0.9779 - val_loss: 0.0893 - val_accuracy: 0.9692\n",
      "Epoch 175/2000\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.0741 - accuracy: 0.9784 - val_loss: 0.0885 - val_accuracy: 0.9685\n",
      "Epoch 176/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0736 - accuracy: 0.9777 - val_loss: 0.0884 - val_accuracy: 0.9692\n",
      "Epoch 177/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0732 - accuracy: 0.9792 - val_loss: 0.0876 - val_accuracy: 0.9715\n",
      "Epoch 178/2000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0724 - accuracy: 0.9790 - val_loss: 0.0876 - val_accuracy: 0.9692\n",
      "Epoch 179/2000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.0738 - accuracy: 0.9777 - val_loss: 0.0871 - val_accuracy: 0.9700\n",
      "Epoch 180/2000\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0729 - accuracy: 0.9787 - val_loss: 0.0930 - val_accuracy: 0.9692\n",
      "Epoch 181/2000\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0748 - accuracy: 0.9782 - val_loss: 0.0978 - val_accuracy: 0.9708\n",
      "Epoch 182/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0767 - accuracy: 0.9769 - val_loss: 0.0872 - val_accuracy: 0.9723\n",
      "Epoch 183/2000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.0743 - accuracy: 0.9782 - val_loss: 0.0862 - val_accuracy: 0.9715\n",
      "Epoch 184/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0710 - accuracy: 0.9787 - val_loss: 0.0923 - val_accuracy: 0.9708\n",
      "Epoch 185/2000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.0751 - accuracy: 0.9784 - val_loss: 0.0842 - val_accuracy: 0.9738\n",
      "Epoch 186/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0703 - accuracy: 0.9802 - val_loss: 0.0867 - val_accuracy: 0.9692\n",
      "Epoch 187/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0704 - accuracy: 0.9792 - val_loss: 0.0875 - val_accuracy: 0.9692\n",
      "Epoch 188/2000\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.0718 - accuracy: 0.9810 - val_loss: 0.0831 - val_accuracy: 0.9754\n",
      "Epoch 189/2000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.0700 - accuracy: 0.9792 - val_loss: 0.0849 - val_accuracy: 0.9738\n",
      "Epoch 190/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0687 - accuracy: 0.9813 - val_loss: 0.0829 - val_accuracy: 0.9762\n",
      "Epoch 191/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0680 - accuracy: 0.9790 - val_loss: 0.0864 - val_accuracy: 0.9700\n",
      "Epoch 192/2000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.0693 - accuracy: 0.9797 - val_loss: 0.0829 - val_accuracy: 0.9777\n",
      "Epoch 193/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0670 - accuracy: 0.9818 - val_loss: 0.0828 - val_accuracy: 0.9746\n",
      "Epoch 194/2000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0697 - accuracy: 0.9797 - val_loss: 0.0863 - val_accuracy: 0.9700\n",
      "Epoch 195/2000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.0685 - accuracy: 0.9810 - val_loss: 0.0822 - val_accuracy: 0.9754\n",
      "Epoch 196/2000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.0666 - accuracy: 0.9826 - val_loss: 0.0820 - val_accuracy: 0.9746\n",
      "Epoch 197/2000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 0.0661 - accuracy: 0.9805 - val_loss: 0.0812 - val_accuracy: 0.9762\n",
      "Epoch 198/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0670 - accuracy: 0.9813 - val_loss: 0.0814 - val_accuracy: 0.9769\n",
      "Epoch 199/2000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.0650 - accuracy: 0.9823 - val_loss: 0.0807 - val_accuracy: 0.9769\n",
      "Epoch 200/2000\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.0651 - accuracy: 0.9823 - val_loss: 0.0791 - val_accuracy: 0.9769\n",
      "Epoch 201/2000\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.0658 - accuracy: 0.9805 - val_loss: 0.0790 - val_accuracy: 0.9762\n",
      "Epoch 202/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0649 - accuracy: 0.9828 - val_loss: 0.0792 - val_accuracy: 0.9769\n",
      "Epoch 203/2000\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.0648 - accuracy: 0.9826 - val_loss: 0.0786 - val_accuracy: 0.9769\n",
      "Epoch 204/2000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.0634 - accuracy: 0.9826 - val_loss: 0.0784 - val_accuracy: 0.9785\n",
      "Epoch 205/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0631 - accuracy: 0.9820 - val_loss: 0.0793 - val_accuracy: 0.9769\n",
      "Epoch 206/2000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.0630 - accuracy: 0.9823 - val_loss: 0.0777 - val_accuracy: 0.9777\n",
      "Epoch 207/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0630 - accuracy: 0.9820 - val_loss: 0.0816 - val_accuracy: 0.9754\n",
      "Epoch 208/2000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.0661 - accuracy: 0.9792 - val_loss: 0.0772 - val_accuracy: 0.9785\n",
      "Epoch 209/2000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 0.0665 - accuracy: 0.9797 - val_loss: 0.0799 - val_accuracy: 0.9762\n",
      "Epoch 210/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0635 - accuracy: 0.9828 - val_loss: 0.0786 - val_accuracy: 0.9754\n",
      "Epoch 211/2000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.0624 - accuracy: 0.9815 - val_loss: 0.0781 - val_accuracy: 0.9777\n",
      "Epoch 212/2000\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0632 - accuracy: 0.9818 - val_loss: 0.0763 - val_accuracy: 0.9785\n",
      "Epoch 213/2000\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.0615 - accuracy: 0.9841 - val_loss: 0.0771 - val_accuracy: 0.9777\n",
      "Epoch 214/2000\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.0640 - accuracy: 0.9820 - val_loss: 0.0822 - val_accuracy: 0.9754\n",
      "Epoch 215/2000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0662 - accuracy: 0.9808 - val_loss: 0.0784 - val_accuracy: 0.9769\n",
      "Epoch 216/2000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 0.0614 - accuracy: 0.9831 - val_loss: 0.0761 - val_accuracy: 0.9777\n",
      "Epoch 217/2000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.0610 - accuracy: 0.9831 - val_loss: 0.0769 - val_accuracy: 0.9792\n",
      "Epoch 218/2000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.0609 - accuracy: 0.9833 - val_loss: 0.0751 - val_accuracy: 0.9777\n",
      "Epoch 219/2000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.0617 - accuracy: 0.9818 - val_loss: 0.0821 - val_accuracy: 0.9746\n",
      "Epoch 220/2000\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.0646 - accuracy: 0.9802 - val_loss: 0.0795 - val_accuracy: 0.9746\n",
      "Epoch 221/2000\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.0616 - accuracy: 0.9843 - val_loss: 0.0762 - val_accuracy: 0.9785\n",
      "Epoch 222/2000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.0619 - accuracy: 0.9841 - val_loss: 0.0776 - val_accuracy: 0.9769\n",
      "Epoch 223/2000\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.0630 - accuracy: 0.9813 - val_loss: 0.0747 - val_accuracy: 0.9792\n",
      "Epoch 224/2000\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0592 - accuracy: 0.9838 - val_loss: 0.0745 - val_accuracy: 0.9785\n",
      "Epoch 225/2000\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.0598 - accuracy: 0.9838 - val_loss: 0.0740 - val_accuracy: 0.9785\n",
      "Epoch 226/2000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.0594 - accuracy: 0.9836 - val_loss: 0.0746 - val_accuracy: 0.9792\n",
      "Epoch 227/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0597 - accuracy: 0.9838 - val_loss: 0.0745 - val_accuracy: 0.9800\n",
      "Epoch 228/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0589 - accuracy: 0.9843 - val_loss: 0.0742 - val_accuracy: 0.9800\n",
      "Epoch 229/2000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.0590 - accuracy: 0.9833 - val_loss: 0.0736 - val_accuracy: 0.9792\n",
      "Epoch 230/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0585 - accuracy: 0.9846 - val_loss: 0.0741 - val_accuracy: 0.9792\n",
      "Epoch 231/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0585 - accuracy: 0.9841 - val_loss: 0.0740 - val_accuracy: 0.9792\n",
      "Epoch 232/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0607 - accuracy: 0.9823 - val_loss: 0.0822 - val_accuracy: 0.9738\n",
      "Epoch 233/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0666 - accuracy: 0.9813 - val_loss: 0.0757 - val_accuracy: 0.9769\n",
      "Epoch 234/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0641 - accuracy: 0.9805 - val_loss: 0.0739 - val_accuracy: 0.9800\n",
      "Epoch 235/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0602 - accuracy: 0.9841 - val_loss: 0.0737 - val_accuracy: 0.9792\n",
      "Epoch 236/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0586 - accuracy: 0.9849 - val_loss: 0.0728 - val_accuracy: 0.9792\n",
      "Epoch 237/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0580 - accuracy: 0.9838 - val_loss: 0.0724 - val_accuracy: 0.9792\n",
      "Epoch 238/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0574 - accuracy: 0.9833 - val_loss: 0.0727 - val_accuracy: 0.9800\n",
      "Epoch 239/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0574 - accuracy: 0.9841 - val_loss: 0.0731 - val_accuracy: 0.9792\n",
      "Epoch 240/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0584 - accuracy: 0.9843 - val_loss: 0.0772 - val_accuracy: 0.9785\n",
      "Epoch 241/2000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 0.0592 - accuracy: 0.9838 - val_loss: 0.0723 - val_accuracy: 0.9800\n",
      "Epoch 242/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0576 - accuracy: 0.9833 - val_loss: 0.0723 - val_accuracy: 0.9800\n",
      "Epoch 243/2000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.0569 - accuracy: 0.9826 - val_loss: 0.0723 - val_accuracy: 0.9800\n",
      "Epoch 244/2000\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 0.0570 - accuracy: 0.9841 - val_loss: 0.0717 - val_accuracy: 0.9792\n",
      "Epoch 245/2000\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0590 - accuracy: 0.9828 - val_loss: 0.0718 - val_accuracy: 0.9792\n",
      "Epoch 246/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0593 - accuracy: 0.9828 - val_loss: 0.0723 - val_accuracy: 0.9785\n",
      "Epoch 247/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0565 - accuracy: 0.9841 - val_loss: 0.0765 - val_accuracy: 0.9785\n",
      "Epoch 248/2000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0579 - accuracy: 0.9836 - val_loss: 0.0714 - val_accuracy: 0.9792\n",
      "Epoch 249/2000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.0582 - accuracy: 0.9843 - val_loss: 0.0714 - val_accuracy: 0.9808\n",
      "Epoch 250/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0578 - accuracy: 0.9833 - val_loss: 0.0712 - val_accuracy: 0.9792\n",
      "Epoch 251/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0563 - accuracy: 0.9836 - val_loss: 0.0712 - val_accuracy: 0.9800\n",
      "Epoch 252/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0562 - accuracy: 0.9846 - val_loss: 0.0713 - val_accuracy: 0.9800\n",
      "Epoch 253/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0557 - accuracy: 0.9843 - val_loss: 0.0723 - val_accuracy: 0.9800\n",
      "Epoch 254/2000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.0561 - accuracy: 0.9838 - val_loss: 0.0711 - val_accuracy: 0.9792\n",
      "Epoch 255/2000\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 0.0564 - accuracy: 0.9843 - val_loss: 0.0709 - val_accuracy: 0.9808\n",
      "Epoch 256/2000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.0578 - accuracy: 0.9838 - val_loss: 0.0747 - val_accuracy: 0.9785\n",
      "Epoch 257/2000\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.0563 - accuracy: 0.9836 - val_loss: 0.0712 - val_accuracy: 0.9800\n",
      "Epoch 258/2000\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.0557 - accuracy: 0.9849 - val_loss: 0.0708 - val_accuracy: 0.9800\n",
      "Epoch 259/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0552 - accuracy: 0.9849 - val_loss: 0.0709 - val_accuracy: 0.9800\n",
      "Epoch 260/2000\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0561 - accuracy: 0.9841 - val_loss: 0.0703 - val_accuracy: 0.9792\n",
      "Epoch 261/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0558 - accuracy: 0.9849 - val_loss: 0.0742 - val_accuracy: 0.9777\n",
      "Epoch 262/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0566 - accuracy: 0.9836 - val_loss: 0.0706 - val_accuracy: 0.9792\n",
      "Epoch 263/2000\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0589 - accuracy: 0.9836 - val_loss: 0.0724 - val_accuracy: 0.9808\n",
      "Epoch 264/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0584 - accuracy: 0.9823 - val_loss: 0.0780 - val_accuracy: 0.9762\n",
      "Epoch 265/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0588 - accuracy: 0.9826 - val_loss: 0.0706 - val_accuracy: 0.9815\n",
      "Epoch 266/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0583 - accuracy: 0.9820 - val_loss: 0.0716 - val_accuracy: 0.9792\n",
      "Epoch 267/2000\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 0.0557 - accuracy: 0.9833 - val_loss: 0.0735 - val_accuracy: 0.9785\n",
      "Epoch 268/2000\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0568 - accuracy: 0.9836 - val_loss: 0.0696 - val_accuracy: 0.9800\n",
      "Epoch 269/2000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0582 - accuracy: 0.9836 - val_loss: 0.0702 - val_accuracy: 0.9800\n",
      "Epoch 270/2000\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.0612 - accuracy: 0.9826 - val_loss: 0.0698 - val_accuracy: 0.9808\n",
      "Epoch 271/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0581 - accuracy: 0.9831 - val_loss: 0.0719 - val_accuracy: 0.9800\n",
      "Epoch 272/2000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.0556 - accuracy: 0.9846 - val_loss: 0.0746 - val_accuracy: 0.9785\n",
      "Epoch 273/2000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0560 - accuracy: 0.9836 - val_loss: 0.0695 - val_accuracy: 0.9792\n",
      "Epoch 274/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0542 - accuracy: 0.9867 - val_loss: 0.0757 - val_accuracy: 0.9762\n",
      "Epoch 275/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0555 - accuracy: 0.9843 - val_loss: 0.0711 - val_accuracy: 0.9815\n",
      "Epoch 276/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0546 - accuracy: 0.9854 - val_loss: 0.0738 - val_accuracy: 0.9785\n",
      "Epoch 277/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0585 - accuracy: 0.9838 - val_loss: 0.0731 - val_accuracy: 0.9800\n",
      "Epoch 278/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0568 - accuracy: 0.9856 - val_loss: 0.0691 - val_accuracy: 0.9785\n",
      "Epoch 279/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0544 - accuracy: 0.9851 - val_loss: 0.0691 - val_accuracy: 0.9792\n",
      "Epoch 280/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0532 - accuracy: 0.9859 - val_loss: 0.0700 - val_accuracy: 0.9792\n",
      "Epoch 281/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0538 - accuracy: 0.9849 - val_loss: 0.0689 - val_accuracy: 0.9792\n",
      "Epoch 282/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0534 - accuracy: 0.9859 - val_loss: 0.0704 - val_accuracy: 0.9792\n",
      "Epoch 283/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0536 - accuracy: 0.9846 - val_loss: 0.0686 - val_accuracy: 0.9792\n",
      "Epoch 284/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0545 - accuracy: 0.9856 - val_loss: 0.0743 - val_accuracy: 0.9792\n",
      "Epoch 285/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0543 - accuracy: 0.9854 - val_loss: 0.0693 - val_accuracy: 0.9815\n",
      "Epoch 286/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0540 - accuracy: 0.9851 - val_loss: 0.0685 - val_accuracy: 0.9785\n",
      "Epoch 287/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0535 - accuracy: 0.9849 - val_loss: 0.0708 - val_accuracy: 0.9792\n",
      "Epoch 288/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0528 - accuracy: 0.9849 - val_loss: 0.0699 - val_accuracy: 0.9831\n",
      "Epoch 289/2000\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0531 - accuracy: 0.9864 - val_loss: 0.0684 - val_accuracy: 0.9792\n",
      "Epoch 290/2000\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 0.0524 - accuracy: 0.9854 - val_loss: 0.0682 - val_accuracy: 0.9792\n",
      "Epoch 291/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0527 - accuracy: 0.9861 - val_loss: 0.0682 - val_accuracy: 0.9785\n",
      "Epoch 292/2000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.0533 - accuracy: 0.9846 - val_loss: 0.0686 - val_accuracy: 0.9815\n",
      "Epoch 293/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0533 - accuracy: 0.9859 - val_loss: 0.0693 - val_accuracy: 0.9831\n",
      "Epoch 294/2000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.0531 - accuracy: 0.9856 - val_loss: 0.0690 - val_accuracy: 0.9800\n",
      "Epoch 295/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0528 - accuracy: 0.9846 - val_loss: 0.0678 - val_accuracy: 0.9785\n",
      "Epoch 296/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0523 - accuracy: 0.9854 - val_loss: 0.0694 - val_accuracy: 0.9831\n",
      "Epoch 297/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0526 - accuracy: 0.9859 - val_loss: 0.0712 - val_accuracy: 0.9815\n",
      "Epoch 298/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0539 - accuracy: 0.9856 - val_loss: 0.0678 - val_accuracy: 0.9785\n",
      "Epoch 299/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0524 - accuracy: 0.9843 - val_loss: 0.0685 - val_accuracy: 0.9823\n",
      "Epoch 300/2000\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0523 - accuracy: 0.9859 - val_loss: 0.0686 - val_accuracy: 0.9823\n",
      "Epoch 301/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0517 - accuracy: 0.9859 - val_loss: 0.0679 - val_accuracy: 0.9785\n",
      "Epoch 302/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0524 - accuracy: 0.9854 - val_loss: 0.0674 - val_accuracy: 0.9785\n",
      "Epoch 303/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0520 - accuracy: 0.9859 - val_loss: 0.0677 - val_accuracy: 0.9815\n",
      "Epoch 304/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0524 - accuracy: 0.9861 - val_loss: 0.0677 - val_accuracy: 0.9815\n",
      "Epoch 305/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0515 - accuracy: 0.9861 - val_loss: 0.0672 - val_accuracy: 0.9785\n",
      "Epoch 306/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0522 - accuracy: 0.9856 - val_loss: 0.0673 - val_accuracy: 0.9792\n",
      "Epoch 307/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0532 - accuracy: 0.9854 - val_loss: 0.0685 - val_accuracy: 0.9792\n",
      "Epoch 308/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0542 - accuracy: 0.9849 - val_loss: 0.0674 - val_accuracy: 0.9785\n",
      "Epoch 309/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0524 - accuracy: 0.9856 - val_loss: 0.0673 - val_accuracy: 0.9800\n",
      "Epoch 310/2000\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0519 - accuracy: 0.9846 - val_loss: 0.0672 - val_accuracy: 0.9785\n",
      "Epoch 311/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0512 - accuracy: 0.9861 - val_loss: 0.0672 - val_accuracy: 0.9800\n",
      "Epoch 312/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0513 - accuracy: 0.9859 - val_loss: 0.0702 - val_accuracy: 0.9823\n",
      "Epoch 313/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0528 - accuracy: 0.9849 - val_loss: 0.0672 - val_accuracy: 0.9815\n",
      "Epoch 314/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0512 - accuracy: 0.9859 - val_loss: 0.0670 - val_accuracy: 0.9815\n",
      "Epoch 315/2000\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0508 - accuracy: 0.9861 - val_loss: 0.0670 - val_accuracy: 0.9785\n",
      "Epoch 316/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0519 - accuracy: 0.9854 - val_loss: 0.0669 - val_accuracy: 0.9808\n",
      "Epoch 317/2000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.0515 - accuracy: 0.9864 - val_loss: 0.0686 - val_accuracy: 0.9838\n",
      "Epoch 318/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0531 - accuracy: 0.9846 - val_loss: 0.0686 - val_accuracy: 0.9846\n",
      "Epoch 319/2000\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0518 - accuracy: 0.9856 - val_loss: 0.0711 - val_accuracy: 0.9815\n",
      "Epoch 320/2000\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0541 - accuracy: 0.9867 - val_loss: 0.0775 - val_accuracy: 0.9762\n",
      "Epoch 321/2000\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.0565 - accuracy: 0.9836 - val_loss: 0.0665 - val_accuracy: 0.9792\n",
      "Epoch 322/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0538 - accuracy: 0.9849 - val_loss: 0.0718 - val_accuracy: 0.9785\n",
      "Epoch 323/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0537 - accuracy: 0.9859 - val_loss: 0.0679 - val_accuracy: 0.9800\n",
      "Epoch 324/2000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.0522 - accuracy: 0.9856 - val_loss: 0.0675 - val_accuracy: 0.9823\n",
      "Epoch 325/2000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.0496 - accuracy: 0.9867 - val_loss: 0.0716 - val_accuracy: 0.9777\n",
      "Epoch 326/2000\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0583 - accuracy: 0.9843 - val_loss: 0.0698 - val_accuracy: 0.9792\n",
      "Epoch 327/2000\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0544 - accuracy: 0.9859 - val_loss: 0.0663 - val_accuracy: 0.9792\n",
      "Epoch 328/2000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.0510 - accuracy: 0.9856 - val_loss: 0.0673 - val_accuracy: 0.9838\n",
      "Epoch 329/2000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.0511 - accuracy: 0.9856 - val_loss: 0.0689 - val_accuracy: 0.9838\n",
      "Epoch 330/2000\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.0525 - accuracy: 0.9854 - val_loss: 0.0659 - val_accuracy: 0.9785\n",
      "Epoch 331/2000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 0.0519 - accuracy: 0.9854 - val_loss: 0.0744 - val_accuracy: 0.9777\n",
      "Epoch 332/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0539 - accuracy: 0.9864 - val_loss: 0.0667 - val_accuracy: 0.9792\n",
      "Epoch 333/2000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 0.0508 - accuracy: 0.9856 - val_loss: 0.0656 - val_accuracy: 0.9815\n",
      "Epoch 334/2000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.0499 - accuracy: 0.9867 - val_loss: 0.0655 - val_accuracy: 0.9808\n",
      "Epoch 335/2000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.0508 - accuracy: 0.9849 - val_loss: 0.0688 - val_accuracy: 0.9831\n",
      "Epoch 336/2000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0510 - accuracy: 0.9877 - val_loss: 0.0684 - val_accuracy: 0.9846\n",
      "Epoch 337/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0527 - accuracy: 0.9859 - val_loss: 0.0661 - val_accuracy: 0.9792\n",
      "Epoch 338/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0528 - accuracy: 0.9864 - val_loss: 0.0709 - val_accuracy: 0.9800\n",
      "Epoch 339/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0532 - accuracy: 0.9859 - val_loss: 0.0662 - val_accuracy: 0.9792\n",
      "Epoch 340/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0544 - accuracy: 0.9856 - val_loss: 0.0697 - val_accuracy: 0.9815\n",
      "Epoch 341/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0534 - accuracy: 0.9854 - val_loss: 0.0716 - val_accuracy: 0.9800\n",
      "Epoch 342/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0518 - accuracy: 0.9846 - val_loss: 0.0663 - val_accuracy: 0.9831\n",
      "Epoch 343/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0498 - accuracy: 0.9867 - val_loss: 0.0656 - val_accuracy: 0.9792\n",
      "Epoch 344/2000\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.0521 - accuracy: 0.9861 - val_loss: 0.0653 - val_accuracy: 0.9792\n",
      "Epoch 345/2000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.0496 - accuracy: 0.9864 - val_loss: 0.0654 - val_accuracy: 0.9823\n",
      "Epoch 346/2000\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0501 - accuracy: 0.9864 - val_loss: 0.0663 - val_accuracy: 0.9831\n",
      "Epoch 347/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0493 - accuracy: 0.9859 - val_loss: 0.0663 - val_accuracy: 0.9800\n",
      "Epoch 348/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0507 - accuracy: 0.9867 - val_loss: 0.0671 - val_accuracy: 0.9800\n",
      "Epoch 349/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0506 - accuracy: 0.9874 - val_loss: 0.0650 - val_accuracy: 0.9800\n",
      "Epoch 350/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0503 - accuracy: 0.9874 - val_loss: 0.0659 - val_accuracy: 0.9823\n",
      "Epoch 351/2000\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.0496 - accuracy: 0.9872 - val_loss: 0.0652 - val_accuracy: 0.9823\n",
      "Epoch 352/2000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.0493 - accuracy: 0.9869 - val_loss: 0.0650 - val_accuracy: 0.9823\n",
      "Epoch 353/2000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0495 - accuracy: 0.9869 - val_loss: 0.0649 - val_accuracy: 0.9815\n",
      "Epoch 354/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0494 - accuracy: 0.9869 - val_loss: 0.0663 - val_accuracy: 0.9831\n",
      "Epoch 355/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0517 - accuracy: 0.9859 - val_loss: 0.0653 - val_accuracy: 0.9823\n",
      "Epoch 356/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0512 - accuracy: 0.9861 - val_loss: 0.0652 - val_accuracy: 0.9800\n",
      "Epoch 357/2000\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.0505 - accuracy: 0.9867 - val_loss: 0.0705 - val_accuracy: 0.9785\n",
      "Epoch 358/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0534 - accuracy: 0.9856 - val_loss: 0.0649 - val_accuracy: 0.9823\n",
      "Epoch 359/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0514 - accuracy: 0.9856 - val_loss: 0.0665 - val_accuracy: 0.9831\n",
      "Epoch 360/2000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.0493 - accuracy: 0.9872 - val_loss: 0.0653 - val_accuracy: 0.9823\n",
      "Epoch 361/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0494 - accuracy: 0.9861 - val_loss: 0.0646 - val_accuracy: 0.9808\n",
      "Epoch 362/2000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.0510 - accuracy: 0.9864 - val_loss: 0.0693 - val_accuracy: 0.9785\n",
      "Epoch 363/2000\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.0523 - accuracy: 0.9846 - val_loss: 0.0646 - val_accuracy: 0.9800\n",
      "Epoch 364/2000\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.0494 - accuracy: 0.9874 - val_loss: 0.0667 - val_accuracy: 0.9838\n",
      "Epoch 365/2000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.0504 - accuracy: 0.9861 - val_loss: 0.0689 - val_accuracy: 0.9831\n",
      "Epoch 366/2000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.0494 - accuracy: 0.9874 - val_loss: 0.0651 - val_accuracy: 0.9800\n",
      "Epoch 367/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0498 - accuracy: 0.9867 - val_loss: 0.0671 - val_accuracy: 0.9800\n",
      "Epoch 368/2000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 0.0503 - accuracy: 0.9861 - val_loss: 0.0642 - val_accuracy: 0.9831\n",
      "Epoch 369/2000\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 0.0501 - accuracy: 0.9877 - val_loss: 0.0655 - val_accuracy: 0.9831\n",
      "Epoch 370/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0486 - accuracy: 0.9867 - val_loss: 0.0642 - val_accuracy: 0.9823\n",
      "Epoch 371/2000\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 0.0494 - accuracy: 0.9869 - val_loss: 0.0647 - val_accuracy: 0.9831\n",
      "Epoch 372/2000\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.0482 - accuracy: 0.9877 - val_loss: 0.0659 - val_accuracy: 0.9831\n",
      "Epoch 373/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0482 - accuracy: 0.9874 - val_loss: 0.0646 - val_accuracy: 0.9808\n",
      "Epoch 374/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0493 - accuracy: 0.9856 - val_loss: 0.0657 - val_accuracy: 0.9800\n",
      "Epoch 375/2000\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.0497 - accuracy: 0.9879 - val_loss: 0.0640 - val_accuracy: 0.9831\n",
      "Epoch 376/2000\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.0496 - accuracy: 0.9864 - val_loss: 0.0691 - val_accuracy: 0.9808\n",
      "Epoch 377/2000\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.0494 - accuracy: 0.9854 - val_loss: 0.0677 - val_accuracy: 0.9815\n",
      "Epoch 378/2000\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.0507 - accuracy: 0.9854 - val_loss: 0.0637 - val_accuracy: 0.9815\n",
      "Epoch 379/2000\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.0478 - accuracy: 0.9882 - val_loss: 0.0639 - val_accuracy: 0.9815\n",
      "Epoch 380/2000\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.0487 - accuracy: 0.9867 - val_loss: 0.0663 - val_accuracy: 0.9831\n",
      "Epoch 381/2000\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 0.0498 - accuracy: 0.9872 - val_loss: 0.0701 - val_accuracy: 0.9800\n",
      "Epoch 382/2000\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 0.0518 - accuracy: 0.9849 - val_loss: 0.0730 - val_accuracy: 0.9792\n",
      "Epoch 383/2000\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.0511 - accuracy: 0.9874 - val_loss: 0.0639 - val_accuracy: 0.9815\n",
      "Epoch 384/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.0496 - accuracy: 0.9851 - val_loss: 0.0646 - val_accuracy: 0.9800\n",
      "Epoch 385/2000\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.0495 - accuracy: 0.9864 - val_loss: 0.0638 - val_accuracy: 0.9823\n",
      "Epoch 386/2000\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.0488 - accuracy: 0.9867 - val_loss: 0.0650 - val_accuracy: 0.9800\n",
      "Epoch 387/2000\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.0492 - accuracy: 0.9851 - val_loss: 0.0641 - val_accuracy: 0.9800\n",
      "Epoch 388/2000\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0484 - accuracy: 0.9874 - val_loss: 0.0660 - val_accuracy: 0.9838\n"
     ]
    }
   ],
   "source": [
    "# 중단 함수를 이용해서 최상의 모델은 찾음\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# 학습이 자동 중단되는 옵션 설정\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "# 모델 저장\n",
    "model_name = './data/model/{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "checkpointer = ModelCheckpoint(filepath=model_name, monitor='val_loss', verbose=0,\n",
    "                               save_best_only=True)\n",
    "\n",
    "# 모델 학습을 2000회 실행\n",
    "model = Sequential()\n",
    "model.add(Dense(24,input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(12,activation='relu'))\n",
    "model.add(Dense(8,activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "hist = model.fit(X_train,y_train, epochs=2000, batch_size=500,\n",
    "                 validation_split=0.25, \n",
    "                 callbacks = [early_stopping, checkpointer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
